{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this assignment, fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`, as well as your name below.\n",
    "\n",
    "To make sure everything runs as expected, do the following\n",
    "- **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart)\n",
    "- **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "A good introduction to Jupyter notebooks is [here](https://realpython.com/jupyter-notebook-introduction/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e67e82f15b9da9033e570c3f93d1485",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HW1 (50 points)\n",
    "\n",
    "In this assignment we will build classifiers to classify a movie review as positive or negative. Using labeled data from IMDb, we will explore how to tokenize each document, create feature vectors, and implement a few different classifiers. Our goal is to understand the overall process of classification using machine learning, as well as to understand how to measure the impact of different algorithmic choices.\n",
    "\n",
    "There are spaces below for you to both write code and short answers. In some places, there are tests to check your work, though passing tests does not guarantee full credit. I recommend moving sequentially from top to bottom, getting each step working before moving on to the next.\n",
    "\n",
    "This assignment will use a number of Python libraries, including `pandas`, `sklearn`, `matplotlib`, `seaborn`, `numpy`, and `scipy`. If you haven't already installed these, you can do so by running this command in this directory: `pip install -r requirements.txt`. Minor variants in the version numbers shouldn't affect things much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "import copy\n",
    "import numpy as np\n",
    "from numpy import array as npa\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5, rc={'figure.figsize':(12, 6)})\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "422752a91419f21fc5c1d3d46dfd41d8",
     "grade": false,
     "grade_id": "cell-a84e17035a9100b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Read and explore data\n",
    "\n",
    "First, we'll read in the data, compute some basic statistics over it, and review some syntax of Pandas.\n",
    "\n",
    "The training data is a tab-separated text file called `train.tsv`. We'll first read it into a Pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). If you haven't used Pandas before, it is a handy library to read and manipulate tabular data. [Here](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) is a nice overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data into a Pandas DataFrame.\n",
    "train_df = pd.read_csv('train.tsv', sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, there are 200 positive and 200 negative documents\n",
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get a column, which is a pandas.Series object, like this:\n",
    "train_df.text  # or: train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's the 11th row\n",
    "train_df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how we can iterate over the DataFrame\n",
    "for rowi, row in train_df.iterrows():\n",
    "    print('row number:', rowi)\n",
    "    print('label:', row['label'])\n",
    "    print('text:', row['text'][:100])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can add a new column by just assigning one.\n",
    "# E.g., here we add a column indicating the character length of each document.\n",
    "train_df['length'] = [len(d) for d in train_df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas has a pretty rich query language we can use to select rows. e.g.:\n",
    "train_df[train_df.length > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[(train_df.length>100) & (train_df.label=='neg')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas supports [matplotlib](https://matplotlib.org/3.3.3/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py) for plotting. E.g., here we plot a histogram of document lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "train_df.length.plot.hist(bins=50)\n",
    "plt.xlabel('length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does document length vary by label? Let's plot and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "train_df[train_df.label=='pos'].length.plot.hist(bins=50, label='pos', alpha=.5)\n",
    "train_df[train_df.label=='neg'].length.plot.hist(bins=50, label='neg', alpha=.5)\n",
    "plt.legend()\n",
    "plt.xlabel('length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...maybe. It does seem like very short documents are more likely to be positive than negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cab9ff0823242d77c86d332cc63f3d7",
     "grade": false,
     "grade_id": "cell-c07b51a18a2af4ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Featurization\n",
    "\n",
    "As we discussed in class, a first step in text classification is converting a document into a **feature vector**.\n",
    "\n",
    "There are many things we can consider:\n",
    "\n",
    "- Do we store word counts or just binary values (1 if word is present, 0 otherwise)?\n",
    "- Do we keep punctuation?\n",
    "- Do we keep capitalization?\n",
    "- Do we just use words or also phrases?\n",
    "\n",
    "There's no \"best\" answer to these questions. It is a tradeoff in the number of unique tokens in our model as well as the frequency with which we see each token in the training data. This will affect things like over/under fitting.\n",
    "\n",
    "Below, complete the `tokenize` function, which takes as input a string representing a document, and returns a list of strings representing each token in the document. Part of your solution will use the `.split()` function of string objects. Then, we have two boolean flags:\n",
    "\n",
    "- `ignore_case`: if True, convert the entire string to lowercase.\n",
    "- `strip_punct`: if True, remove any leading or trailing punctuation for each token. E.g., \"!it's?\" would become \"it's\". You can use Python's [regular expression library](https://docs.python.org/3/library/re.html) library to do so. Hint: consider using the `sub` method combined with the `\\w` and `\\W` word classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55b53512ab22f019f6d610e6511ebc79",
     "grade": false,
     "grade_id": "tokenize",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(document, strip_punct=True, ignore_case=True):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3d62d3bb726ebada3f3bea74b461aa1",
     "grade": true,
     "grade_id": "tokenize-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert tokenize(\" Hi there! Isn't this fun?\", strip_punct=True, ignore_case=True) == ['hi', 'there', \"isn't\", 'this', 'fun']\n",
    "assert tokenize(\" Hi there! Isn't this fun?\", strip_punct=False, ignore_case=True) == ['hi', 'there!', \"isn't\", 'this', 'fun?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6ddf5ec38135d74502cfa91d60c07b1",
     "grade": false,
     "grade_id": "cell-ffe48966997b3a33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we can choose a specific tokenization setting and apply it to all documents.\n",
    "\n",
    "We'll store the results in a new column called `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = [tokenize(d, strip_punct=True, ignore_case=True) for d in train_df.text]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a feature vector for each document. For now, we'll assume **binary features**, which means for each document we'll store a `dict` where words are keys and values are 1 for each word that exists in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5cf6d0825f0856cca6c4f06ca4a5f03",
     "grade": false,
     "grade_id": "featurize",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def featurize(tokens):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a33b71afaa65c05d1c0c54ae305200dc",
     "grade": true,
     "grade_id": "featurize-test",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "feats = featurize(tokenize(\" Hi there! Isn't this fun? Hi there\", strip_punct=True, ignore_case=True))\n",
    "print(feats)\n",
    "assert sorted(feats.items()) == [('fun', 1), ('hi', 1), (\"isn't\", 1), ('there', 1), ('this', 1)]\n",
    "# note that sets and dicts are unordered; i'm sorting by alpha for testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can featurize all the documents and assign to a new column called `raw_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['raw_features'] = [featurize(t) for t in train_df.tokens]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common issue in text classification is that many words occur infrequently. This poses a challenge to any machine learning method -- if we've only seen a word once, we cannot be very confident about whether it correlates with the positive or negative class! First, let's count word frequencies by completing the method below. It takes in a list of `dict` objects, from the `raw_features` column, and returns a [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) object representing the number of documents each word appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78abc6a7187f2e960ba6f3952db2ae1a",
     "grade": false,
     "grade_id": "count-words",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_word_document_frequency(dict_list):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "word_counts = count_word_document_frequency(train_df.raw_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should have 12,166 unique words\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can print the most common entries in a Counter like so:\n",
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6a8c634c46533ff04bd69cb90c15e1e",
     "grade": true,
     "grade_id": "count-words-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert count_word_document_frequency(train_df.raw_features).most_common(5) == [('the', 399), ('and', 392), ('a', 391), ('to', 380), ('of', 376)]\n",
    "\n",
    "assert len(count_word_document_frequency(train_df.raw_features)) == 12166"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just how common are rare words? Let's plot a histogram to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(word_counts.values(), bins=100)\n",
    "plt.xlabel('number of documents a word appears in')\n",
    "plt.ylabel('number of words')\n",
    "plt.show()\n",
    "Counter(word_counts.values()).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whoa!** There are 7,571 out of 12,166 words that occur exactly once, and 1,698 that occur exactly twice.\n",
    "\n",
    "This is very common in text collections. There are very many rare words and a small number of very common terms. This can sometimes be better seen in a `log-log` plot. These data somewhat follow something known as a [Power Law](https://en.wikipedia.org/wiki/Power_law) distribution, which just means this log-log plot is well-approximated by a linear fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-log plot of word frequencies\n",
    "def log_log_plot(word_counts):\n",
    "    plt.figure()\n",
    "    plt.loglog(sorted(word_counts.values())[::-1])\n",
    "    plt.xlabel('word rank')\n",
    "    plt.ylabel('number of documents a word appears in')\n",
    "    plt.show()\n",
    "    \n",
    "log_log_plot(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That long, horizontal segment at the bottom right of the plot represents the 7,571 words that occur exactly once.\n",
    "\n",
    "The words at the top left are the most common words (`the`, `and`, etc.)\n",
    "\n",
    "It seems that neither very common nor very rare words should be informative in our model.\n",
    "- For very rare words, we don't see enough examples to have much confidence in our probability estimates.\n",
    "- For very common words, if they appear in just about every document, they probably do not correlate with the class label.\n",
    "\n",
    "We will next create a `vocabulary`, which contains our final set of unique features. To do so, we will remove terms that occur too frequently or too infrequently.\n",
    "\n",
    "The inputs to `create_vocabulary` are:\n",
    "- word_counts: the Counter object compute above\n",
    "- min_count: minimum document count allowed\n",
    "- max_count: maximum document count allowed\n",
    "\n",
    "This function should return a `dict` where each key is a word and the value is a unique `int` representing the identifier for that word. We will assign each word its value in alphabetical order, e.g. `{'aardvark': 0, 'beetle': 1, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "543ee07eac232b58cb4fa22ecc8e780f",
     "grade": false,
     "grade_id": "prune-words",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(word_counts, min_count=2, max_count=100):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "vocabulary = create_vocabulary(word_counts, min_count=2, max_count=100)\n",
    "list(vocabulary.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27b99c08b728700a04e0828dd29da78f",
     "grade": true,
     "grade_id": "prune-words-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(vocabulary) == 4519\n",
    "assert list(vocabulary.items())[:10] == [('0', 0), ('1', 1), ('1/2', 2), ('10', 3), ('10/10', 4), ('100', 5), ('11', 6), ('12', 7), ('15', 8), ('16', 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finalized our vocabulary, we need to go back through all our `raw_features` in `train_df` and remove any that are not in this `vocabulary`. We store the result in a new column called `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_features(vocabulary, raw_feature_dict):\n",
    "    return {k:v for k,v in raw_feature_dict.items() if k in vocabulary}\n",
    "\n",
    "train_df['features'] = [prune_features(vocabulary, f) for f in train_df.raw_features]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just as a sanity check, let's plot the word frequency distribution again.\n",
    "word_counts_pruned = count_word_document_frequency(train_df.features)\n",
    "log_log_plot(word_counts_pruned)\n",
    "# yes, we've chopped off the left and right part of the original graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "OK, now we're finally ready to fit a classifier. Let's start with Bernoulli Naive Bayes. Recall the formula to compute the word probabilities for each class, using smoothing:\n",
    "\n",
    "$$p(x_{k}=1|y=1) = \\frac{\\epsilon + \\sum_{(x_i, y_i) \\in D}1[x_{ik}=1 \\wedge y_i=1]}{2 \\epsilon + \\sum_{(x_i, y_i) \\in D} 1[y_i=1]}$$\n",
    "\n",
    "Commonly, $\\epsilon=1$ is used (“plus one” smoothing). \n",
    "\n",
    "That is, the probability that word $k$ is present in a positive document is the fraction of positive documents that contain word $k$, modulo the smoothing terms.\n",
    "\n",
    "To compute this, we'll first create a `dict` where keys are words and values are `p(x_{k}=1|y=1)`. We'll create a separate `dict` for $y=$pos and $y=$neg. You should be able to reuse your `count_word_document_frequency` method to help with this.\n",
    "\n",
    "**note**: be sure to account for words that only appear in one class. These would only have value $\\epsilon$ in the numerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1bda08c30b66746834f0cfc3a337fec",
     "grade": false,
     "grade_id": "p_x_given_y",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def p_x_given_y(train_df, vocabulary, label, epsilon=0):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "p_x_given_pos = p_x_given_y(train_df, vocabulary, 'pos', epsilon=1)\n",
    "p_x_given_neg = p_x_given_y(train_df, vocabulary, 'neg', epsilon=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e516fd7a1c396f25022e90789314651",
     "grade": true,
     "grade_id": "p_x_given_y-test",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(p_x_given_neg['bad'], 3) == 0.307\n",
    "assert round(p_x_given_pos['bad'], 3) == 0.139\n",
    "\n",
    "# sanity check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the top values for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('top p(x|pos)')\n",
    "print(sorted(p_x_given_pos.items(), key=lambda x: -x[1])[:15])\n",
    "\n",
    "print('top p(x|neg)')\n",
    "print(sorted(p_x_given_neg.items(), key=lambda x: -x[1])[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to see things like `great` and `best` in the positive class and `bad` in the negative class, but both distributions are dominated by very common terms -- e.g., `too` is very common in both classes.\n",
    "\n",
    "To get a better sense of the relative frequency by class, we can simply subtract $p(x|pos)-p(x|neg)$ to find terms that are relatively more frequent in the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = pd.DataFrame([{'word':w, 'pos_score': p_x_given_pos[w] - p_x_given_neg[w]} for w in vocabulary])\n",
    "print('positive terms')\n",
    "display(word_scores.sort_values('pos_score', ascending=False).head(10))\n",
    "\n",
    "print('negative terms')\n",
    "display(word_scores.sort_values('pos_score', ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These make more sense, though notice we still have some surprising words in the top 10 (e.g., `also`, `man`).\n",
    "\n",
    "Let's stop and think some more about what smoothing is doing. Below we try different values of $\\epsilon$ and print the top 5 terms for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20]:\n",
    "    ppp = p_x_given_y(train_df, vocabulary, 'pos', epsilon=e)\n",
    "    ppp = sorted(ppp.items(), key=lambda x: -x[1])\n",
    "    print(ppp[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\epsilon$ increases to infinity, what value will $p(x|y)$ converge to? To answer this, just return the value in the method below (e.g., `return 0` or `return 100`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "528827746005dfbd89500fc7acfdf938",
     "grade": false,
     "grade_id": "convergence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def convergence_value():\n",
    "    # return the float value that you think p(x|y) converges to as epsilon approaches infinity.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bbfb3bc5d6923112cf42f9c636f6b10",
     "grade": true,
     "grade_id": "convergence-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about this result and how it relates to the event model of Bernoulli Naive Bayes. E.g., a coin flip for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other quantity we need is $p(y=1)$, the prior probability. Since this is a binary classification task, we know that $p(y=0) = 1-p(y=1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b571161ea05b04c081b23dba954abf66",
     "grade": false,
     "grade_id": "prior",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_prior(df):\n",
    "    \"\"\"\n",
    "    Compute the prior probability p(y=1) given a training set DataFrame.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "prior = compute_prior(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0104d580322e043c1a0ad7733a31919b",
     "grade": true,
     "grade_id": "prior-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(compute_prior(train_df), 1) == 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's actually classify a document using Naive Bayes. Recall the classification formula:\n",
    "\n",
    "$$\n",
    "p(y=1|\\vec{x}) = \\frac{p(y=1)\\prod_j p(x_{ij}|y=1)}{p(\\vec{x})}\n",
    "$$\n",
    "\n",
    "The product in the numerator must loop over all words in the vocabulary. Recall that:\n",
    "\n",
    "$$\n",
    "p(x_{ij}=0 \\mid y=1) = 1 - p(x_{ij}=1 \\mid y=1)\n",
    "$$\n",
    "\n",
    "Also, to prevent underflow, we'll have to use this equivalence:\n",
    "$$\n",
    "\\log(p(y=1|\\vec{x})) = \\log(p(y=1)) + \\sum_j \\log p(x_{ij}|y=1) - \\log(p(\\vec{x}))\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\log(p(x)) = \\log \\Big( p(\\vec{x}|y=1)p(y=1) + p(\\vec{x}|y=0)p(y=0) \\Big)$$\n",
    "\n",
    "and to get the final answer:\n",
    "$$p(y=1|\\vec{x}) = \\exp \\Big( \\log(p(y=1|\\vec{x})) \\Big)$$\n",
    "(using `math.log` and `math.exp`)\n",
    "\n",
    "\n",
    "Below, implement `log_p_y_given_x_numerator` which computes just the numerator:\n",
    "\n",
    "$$\\log(p(y)) + \\sum_j \\log p(x_{ij}|y)$$\n",
    "\n",
    "Depending on what is passed in for `p_x_given_y` and `prior`, this will compute the numerator either for $y=1$ or $y=0$.\n",
    "\n",
    "This function is then used by `pr_pos_given_x`, which is done for you, to compute $p(y=1|x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e85f6b626626c0e32db8c5b5148abf4d",
     "grade": false,
     "grade_id": "classify-nb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_p_y_given_x_numerator(features, p_x_given_y, prior, vocabulary):\n",
    "    \"\"\"\n",
    "    returns log(p(y)) + \\sum_j \\log p(x_j|y). This is the numerator in the equation\n",
    "    for p(y|x) above.\n",
    "    \n",
    "    note that p_x_given_y and prior can be values for y=1 or y=0, depending on what\n",
    "    is passed in. see usage in pr_pos_given_x below.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def pr_pos_given_x(features, p_x_given_pos, p_x_given_neg, prior, vocabulary):\n",
    "    \"\"\"\n",
    "    Returns the probability p(y=1|x). This is complete and should not need to be modified.\n",
    "    \"\"\"\n",
    "    pos_numerator = log_p_y_given_x_numerator(features, p_x_given_pos, prior, vocabulary)\n",
    "    neg_numerator = log_p_y_given_x_numerator(features, p_x_given_neg, 1-prior, vocabulary)\n",
    "    # there's an additional log-sum-exp trick to avoid underflow when computing p(x)\n",
    "    # https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\n",
    "    maxv = max((pos_numerator, neg_numerator))\n",
    "    log_p_x = maxv + math.log(math.exp(pos_numerator-maxv) + math.exp(neg_numerator-maxv))\n",
    "    v = pos_numerator - log_p_x\n",
    "    return math.exp(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "513974b69471d3354a9680e76b2b9679",
     "grade": true,
     "grade_id": "classify-nb-test",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(pr_pos_given_x({'great': 1, 'most': 1, 'best': 1, 'well': 1}, p_x_given_pos, p_x_given_neg, prior, vocabulary), 2) == 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1599e61615fe27ac4849bcca3cb9719",
     "grade": true,
     "grade_id": "classify-nb-test-2",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(pr_pos_given_x({'bad': 1, 'worst': 1, 'terrible': 1}, p_x_given_pos, p_x_given_neg, prior, vocabulary), 3) == 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to read in the testing data, compute features, and classify.\n",
    "\n",
    "Here, we have to be very careful to process the data in the exact same way, to ensure our feature set is the same in both training and testing. This means the tokenizer is the same, and the features should be pruned to those in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.tsv', sep='\\t')\n",
    "test_df['tokens'] = [tokenize(d, strip_punct=True, ignore_case=True) for d in test_df.text]\n",
    "test_df['raw_features'] = [featurize(t) for t in test_df.tokens]\n",
    "test_df['features'] = [prune_features(vocabulary, f) for f in test_df.raw_features]\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute $p(y=1|x)$ for each training and testing instance. We'll store the result in a new column called `pr_pos`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pr_pos'] = [pr_pos_given_x(f, p_x_given_pos, p_x_given_neg, prior, vocabulary)\n",
    "                      for f in train_df.features]\n",
    "test_df['pr_pos'] = [pr_pos_given_x(f, p_x_given_pos, p_x_given_neg, prior, vocabulary)\n",
    "                     for f in test_df.features]\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll assign the predicted label as positive if the probability is $\\ge .5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['predicted_label'] = ['pos' if v >= .5 else 'neg' for v in train_df.pr_pos]\n",
    "test_df['predicted_label'] = ['pos' if v >= .5 else 'neg' for v in test_df.pr_pos]\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute some quality metrics over the test set. We'll use [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) method from the sklearn library. Please read through the documentation to understand what these metrics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('results on training data')\n",
    "print(classification_report(train_df.label, train_df.predicted_label))\n",
    "print('results on testing data')\n",
    "print(classification_report(test_df.label, test_df.predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the overall accuracy is around 78%. Not bad for a simple method!\n",
    "\n",
    "The precision and recall differ a bit by class, with `pos` having higher precision but lower recall. Based on these results, what is the more common type of error: classifying a negative document as positive (false positive) or classifying a positive document as negative (false negative)? **Submit your answer** by returning either \"false positive\" or \"false negative\" in the method below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e81b3713653da78c613115025a62e8e1",
     "grade": false,
     "grade_id": "error-type",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def error_type():\n",
    "    # return either \"false positive\" or \"false negative\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aacf577ec860d2c389a3d4b4c97f4dc8",
     "grade": true,
     "grade_id": "error-type-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Next we'll look more closely at Logistic Regression. Below is the code from class to perform gradient descent, using negative log likelihood as the error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient_fn, error_fn, theta,\n",
    "                     learning_rate, D, tolerance, max_iters):\n",
    "    errori = error_fn(theta, D)\n",
    "    iters = 0\n",
    "    trace = [] # for debugging\n",
    "    while True:\n",
    "        iters += 1\n",
    "        theta_cp = copy.copy(theta)\n",
    "        print('\\n\\niteration %d' % iters)\n",
    "        grad = gradient_fn(theta, D)\n",
    "        trace.append((theta.copy(), grad, errori))\n",
    "        print('gradient=', grad)\n",
    "        theta -= learning_rate * grad  # UPDATE!\n",
    "        newerror = error_fn(theta, D)\n",
    "        print('old error=%g   new error=%g  theta=%s\\n\\n' %\n",
    "              (errori, newerror, str(theta)))\n",
    "        error_diff = errori - newerror\n",
    "        # stopping criteria\n",
    "        if error_diff < 0:\n",
    "            learning_rate *= .5\n",
    "            print('error got worse. reducing learning rate to %g' % learning_rate)\n",
    "            theta = theta_cp\n",
    "            errori = error_fn(theta, D)\n",
    "        elif errori - newerror < tolerance:\n",
    "            print('error change is too small')\n",
    "            break\n",
    "        elif iters >= max_iters:\n",
    "            print('max iterations reached')\n",
    "            break\n",
    "        else:\n",
    "            errori = newerror\n",
    "    trace = pd.DataFrame(trace, columns=['theta', 'gradient', 'error'])\n",
    "    display(trace)\n",
    "    plt.plot(trace.error, 'bo-')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('error')\n",
    "    return theta\n",
    "\n",
    "def f(x, theta):\n",
    "    # dot product\n",
    "    return x.dot(theta)\n",
    "\n",
    "def logistic(x, theta):\n",
    "    # logistic function: p(y=1|x)\n",
    "    return 1 / (1 + math.exp(-f(x, theta)))\n",
    "\n",
    "def nll(theta, D):\n",
    "    # negative log likelihood\n",
    "    total = 0\n",
    "    predictions = [] # for debugging\n",
    "    for xi, yi in D:\n",
    "        pred = logistic(xi, theta) if yi==1 else 1-logistic(xi, theta)\n",
    "        total += math.log(pred)\n",
    "        predictions.append((xi, yi, pred, 1-pred))\n",
    "    display(pd.DataFrame(predictions, columns=['x', 'y', 'prediction', 'error']))        \n",
    "    return -total\n",
    "\n",
    "def gradient_logistic(theta, D):\n",
    "    # gradient function for logistic regression\n",
    "    # updated from lecture to use csr_matrix as feature vectors, \n",
    "    # instead of numpy arrays.\n",
    "    result = np.zeros(len(theta), dtype=np.float64)\n",
    "    for xi, yi in D:\n",
    "        p_y_g_x = logistic(xi, theta) if yi==1 else 1-logistic(xi, theta)\n",
    "        error = yi * (1-p_y_g_x)\n",
    "        for j, xij in zip(xi.indices, xi.data):\n",
    "            result[j] += error * xij\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code expects the features of a document to be a `numpy array`, rather than a `dict` like we used in naive bayes.\n",
    "\n",
    "Complete the code below, which creates a numpy array from a feature `dict`. The array should have `1` at location `i` if word `i` is present in the feature dictionary. The order of the array is deterined by the `vocabulary` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b6d76e77c4f2627381ab7b8f1a307d4",
     "grade": false,
     "grade_id": "fv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def features2array(features, vocabulary):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecbb5a26c9b3f3e2edb50c53b64aebcf",
     "grade": true,
     "grade_id": "fv-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "feature_vector = features2array({'great': 1, 'terrible': 1}, vocabulary)\n",
    "assert len(feature_vector) == len(vocabulary)\n",
    "assert feature_vector[vocabulary['great']] == 1.0\n",
    "assert feature_vector[vocabulary['terrible']] == 1.0\n",
    "assert feature_vector[vocabulary['also']] == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector representation is very space inefficient. Since most documents only use a small subset of the full vocabulary, most values will be zero in the feature vector. E.g., below is the number of 0 and 1 values stored in the first training document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(features2array(train_df.features[0], vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will instead use a [`csr_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html), which is a sparse representation of an array. It only stores the non-zero values, along with some indices keeping track of which column each non-zero value correponds to. Below, I've given you the function to do this. We can see that it saves about 33kb just for the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features2sparse_array(features, vocabulary):\n",
    "    return csr_matrix(features2array(features, vocabulary), shape=(1, len(vocabulary)))\n",
    "\n",
    "dense_array = features2array(train_df.features[0], vocabulary)\n",
    "sparse_array = features2sparse_array(train_df.features[0], vocabulary)\n",
    "print('document contains %d/%d words' % (sparse_array.nnz, len(vocabulary)))\n",
    "print('dense array requires %d bytes, sparse array requires %d bytes' % \n",
    "      (dense_array.nbytes, sparse_array.data.nbytes + sparse_array.indices.nbytes + sparse_array.indptr.nbytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will add these sparse arrays to our training and testing DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['feature_vector'] = [features2sparse_array(f, vocabulary) for f in train_df.features]\n",
    "test_df['feature_vector'] = [features2sparse_array(f, vocabulary) for f in test_df.features]\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the full training dataset $D$, which is a list of tuples of the form (feature_vector, label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [(fv, 1 if label=='pos' else -1) for label, fv in train_df[['label', 'feature_vector']].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create an initial $\\theta$ vector of 0s and call gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(len(vocabulary))\n",
    "theta = gradient_descent(gradient_logistic, nll, theta, .3, D, .01, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy on training and testing data\n",
    "print('results on training data')\n",
    "train_df['pr_pos_lr'] = [logistic(features2sparse_array(f, vocabulary), theta) for f in train_df.features]\n",
    "train_df['predicted_label_lr'] = ['pos' if v >= .5 else 'neg' for v in train_df.pr_pos_lr]\n",
    "print(classification_report(train_df.label, train_df.predicted_label_lr))\n",
    "\n",
    "print('results on testing data')\n",
    "test_df['pr_pos_lr'] = [logistic(features2sparse_array(f, vocabulary), theta) for f in test_df.features]\n",
    "test_df['predicted_label_lr'] = ['pos' if v >= .5 else 'neg' for v in test_df.pr_pos_lr]\n",
    "print(classification_report(test_df.label, test_df.predicted_label_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classifier has perfect accuracy on the training data, and similar accuracy as naive bayes on the test data.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Now, let's inspect the $\\theta$ coefficients. Here are the largest and smallest values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top coefficients for each class.\n",
    "reverse_vocab = {i:v for v,i in vocabulary.items()}\n",
    "for i in np.argsort(theta)[::-1][:15]:\n",
    "    print(reverse_vocab[i], theta[i])\n",
    "print()\n",
    "for i in np.argsort(theta)[:15]:\n",
    "    print(reverse_vocab[i], theta[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting coefficients is a good way to understand your data better. For example, the word \"1\" appears to be strongly associated with the negative class. Do some digging in the original data to figure out why. In what context does the number \"1\" appear, and why is it correlated with the negative class? \n",
    "\n",
    "**Write your answer in the cell below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea2b425d55c3f0ea6f31cf639bcfb9f1",
     "grade": true,
     "grade_id": "number-one",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, how should we interpret these coefficients? What does a coefficient of $3.98$ tell us about the term `great`?\n",
    "\n",
    "In linear regression, we know that the coefficient $\\theta_i$ represents the strength of the linear relationship between the independent and dependent variables. That is, as the independent variable ($x_i$) increases by one unit, we expect the dependent variable ($y$) to increase by $\\theta_i$.\n",
    "\n",
    "This is a bit more complicated for logistic regression, since the logistic function introduces a non-linear relationship between $x$ and $y$.\n",
    "\n",
    "One approach is to just pass the coefficient through the logistic function. This tells us the probability that a document containing just this single word is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_single(x):\n",
    "    return 1 / (1+math.exp(-x))\n",
    "\n",
    "\n",
    "print(\"p(y=1|great)=%.3f\" % logistic_single(theta[vocabulary['great']]))\n",
    "print(\"p(y=1|terrible)=%.3f\" % logistic_single(theta[vocabulary['terrible']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn\n",
    "\n",
    "Now, we did all this the hard way to understand how this works. Of course, there are libraries that do most of these steps for us. E.g., [`sklearn`](https://scikit-learn.org/stable/index.html) is a popular machine learning library; a good tutorial is [here](https://www.datacamp.com/community/tutorials/machine-learning-python).\n",
    "\n",
    "For example, the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer) class in sklearn provides a way to extract tokens and features from raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_df=100, min_df=2, binary=True)\n",
    "X_train = vec.fit_transform(train_df.text)\n",
    "X_test = vec.transform(test_df.text)\n",
    "print('X_train is a csr_matrix with %d rows and %d columns' % (X_train.shape[0], X_train.shape[1]))\n",
    "print('vec.vocabulary_ maps words to indices. E.g., \"great\" has index %d' % vec.vocabulary_['great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn has a number of classifiers implemented, including [Bernoulli](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html?highlight=bernoulli#sklearn.naive_bayes.BernoulliNB) and [Multinomial](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomial#sklearn.naive_bayes.MultinomialNB) Naive Bayes, and [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB(alpha=1)\n",
    "nb.fit(X_train, train_df.label)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(classification_report(test_df.label, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is pretty close to what we got above. There are some small differences in the vocabulary.\n",
    "\n",
    "We can find the word probabilities in `nb.feature_log_prob`, which stores $\\log p(x_i \\mid y)$ for each class. We can exponentiate to get the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these should be similar to what we calculated above in our own implementation.\n",
    "great_idx = vec.vocabulary_['great']\n",
    "print('p(great|y=1)=%.2f' % math.exp(nb.feature_log_prob_[1][great_idx]))\n",
    "print('p(great|y=0)=%.2f' % math.exp(nb.feature_log_prob_[0][great_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly fit LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we didn't use any regularization in our implementation above,\n",
    "# I'm setting C to a large number to reduce the effect of the L2 regularizer.\n",
    "lr = LogisticRegression(C=1e10)\n",
    "lr.fit(X_train, train_df.label)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "print(classification_report(test_df.label, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is pretty close to our implementation above. \n",
    "\n",
    "We can find the $\\theta$ coefficients in `lr.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('theta for \"great\" is %.3f' % lr.coef_[0][great_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering\n",
    "\n",
    "Now that you understand a bit more about logistic regression and naive bayes, explore `sklearn` for a while to see if you can come up with an approach that has higher test accuracy than we've shown above. A few ground rules:\n",
    "\n",
    "- You can use MultinomialNB, BernoulliNB, or LogisticRegression.\n",
    "- You can modify the tokenization and featurization steps in any way you like.\n",
    "- You can explore any parameters to the constructors of any of the classifiers.\n",
    "\n",
    "In the code cells below, try out different settings to see how it affects test accuracy.\n",
    "\n",
    "In the final written cell, briefly summarize what options you explored, what worked best, what the accuracy was, and why you think your choices improved accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f7e6b2fd703e8d53bfed2f1fd844c74",
     "grade": true,
     "grade_id": "engineering",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
